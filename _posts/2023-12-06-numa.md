---
layout: post
title: "numa"
category: qemu
tags: [qemu,libvirt,numa虚拟化,cpu虚拟化,内存虚拟化]
---
{% include JB/setup %}


本文通过对qemu/kvm虚拟机设置numa探讨numa架构、guest中对numa、cpu、memory设置。

# NUMA

> **Non-uniform memory access** (**NUMA**) is a [computer memory](https://en.wikipedia.org/wiki/Computer_storage) design used in [multiprocessing](https://en.wikipedia.org/wiki/Multiprocessing), where the memory access time depends on the memory location relative to the processor.

- NUMA架构下，CPU所属某node(numa节点)。memory可以所属某node，称之为该node的local memory。也可以不属于任何node，称之为non-local memory。

- 和CPU处于同一个node的memory，我们称之为该cpu的关联memory。其他的memory称之为非关联memory，包括不同node的local memory和non-local memory。CPU访问关联memory的效率非常高，而访问非关联memory的效率低的多。

- system ram是local memory。pci bus**可以**所属某node，当pci bus所属某node时，该bus下的所有pci device相关的memory都是local memory。对于系统中其他的memory通常为non-local memory。
- CPU、memory和node的关系由硬件决定，具体来说由设备插入主板上的插槽决定。



## lantency、...

和numa相关的概念还有lantency、bandwidth、initiator、numa cache、distance、HMAT(ACPI表)，具体查看相关手册。



## numa in linux

### sysfs

每个node对应一个/sys/devices/system/node/下的子目录，其目录下的文件记录了node的具体信息。

对于pci device，其对应的sysfs目录下的文件numa_node记录了其所属的node，-1表示pci device不属于任何node。

![image-20231205173149976](https://raw.githubusercontent.com/Reventon1993/pictures/master/typora/numa_pic1.png)



### numa policy&numactl

具体man numactl

### numa.h

用户编程接口，具体man 3 numa

### numastat

查看进程对node的memory的使用情况。



# NUMA IN QEMU

## -numa

```
-numa node[,mem=size][,cpus=firstcpu[-lastcpu]][,nodeid=node][,initiator=initiator]

-numa node[,memdev=id][,cpus=firstcpu[-lastcpu]][,nodeid=node][,initiator=initiator]

-numa dist,src=source,dst=destination,val=distance

-numa cpu,node-id=node[,socket-id=x][,core-id=y][,thread-id=z]

-numa hmat-lb,initiator=node,target=node,hierarchy=hierarchy,data-type=tpye[,latency=lat][,bandwidth=bw]

-numa hmat-cache,node-id=node,size=size,level=level[,associativity=str][,policy=str][,line=size]
```

我们着重讨论-numa参数对cpu、memory的设置，除此还可以设置initiator、distance、latency、bandwidth、numa cache。

-numa参数只对-smp、-m等参数设置的cpu、memory进行node的关联，本身不分配新的cpu、memory。



## -smp&-cpu

```
-smp [[cpus=]n][,maxcpus=maxcpus][,sockets=sockets][,dies=dies][,cores=cores][,threads=threads]
```

- smp参数中，cpus表示开机时的cpu数量，maxcpus表示运行时最大的cpu数量。
- maxcpus = sockets\*dies\*cores\*threads
- die是个不太常见的概念，它表示芯片块，在linux系统中cores表示这里的dies\*cores。
- 在-numa参数中可以对[0,maxcpus-1]的cpu关联node。如果没有指定，那么cpu默认所属node0。



-cpu指定cpu的型号和feature

```
-cpu model
```



## -m

```
-m [size=]megs[,slots=n,maxmem=size]
```

- size表示开机时system ram的大小。

- slots表示hotpluggable memory(ram)的最大数量。

- maxmem表示运行时最大的system ram+hotpluggable memory的大小。

- -numa中mem/memdev指定开机时system ram的大小，其与-m的size存在一致性关系。

- -num中mem和memdev是互斥的，memdev提供对内存更精细的控制。

  

### hotpluggable memory

-device pc-dimm表示一个ram设备(开机时就就存在)，该设备的总线为dimm，-m的slots指定了dimm总线的插槽数。

后续可以使用hmp的device_add命令热添加一个pc-dimm设备，因此pc-dimm对应的内存为hotpluggable memory。

-device pc-dimm需指定其所属的node(hotpluggable memory是local memory)

### 示例

system ram: node0 1G + node1 1G

hotpluggable memory: 1G in node0

```
-m size=2097152k,slots=1,maxmem=269484032k \
-numa node,nodeid=0,cpus=0,mem=1024 \
-numa node,nodeid=1,cpus=1,mem=1024 \
-object memory-backend-ram,id=memdimm0,size=1073741824 \
-device pc-dimm,node=0,memdev=memdimm0,id=dimm0 \
```



system ram: node0 1G+ node1 1G

```
-m 2G,slots=2,maxmem=4G \
-object memory-backend-ram,size=1G,id=m0 \
-object memory-backend-ram,size=1G,id=m1 \
-numa node,nodeid=0,memdev=m0 \
-numa node,nodeid=1,memdev=m1 \
```



## -device pxb

pxb表示一个pci-expander-bus controller(一种pci controller)。其支持numa架构，可以所属一个node。numa_node指定其所属node。该pci总线下的pci device所属该node。



示例：

```
-device pxb,bus_nr=254,id=pci.1,numa_node=1,bus=pci.0,addr=0x6
-device virtio-balloon-pci,id=balloon0,bus=pci.1,addr=0x6 -msg timestamp=on
```

<img src="https://raw.githubusercontent.com/Reventon1993/pictures/master/typora/numa_pic2.png" alt="image-20231206145545441"  />



# libvirt xml

```
<vcpu placement='static' current='4'>128</vcpu>
```

current对应-smp中cpus，vcpu的值(128)对应-smp的maxcpus



```
<cpu mode='host-model' check='partial'>
  <model fallback='allow'/>
  <topology sockets='2' cores='64' threads='1'/>
  <cache level='3' mode='emulate'/>
  <numa>
    <cell id='0' cpus='0' memory='1048576' unit='KiB'/>
    <cell id='1' cpus='1' memory='1048576' unit='KiB'/>
  </numa>
</cpu>
```

- \<topology\>对应-smp的sockets、dies、cores、threads
- \<numa\>对应-numa参数中memory、cpu部分
- mode等参数对应-cpu。



```
<maxMemory slots='256' unit='KiB'>3145728</maxMemory>
<memory unit='KiB'>3145728</memory>
<currentMemory unit='KiB'>3145728</currentMemory>
```

- \<memory\>对应-m参数的size
- \<maxMemory\>对应-m的slots和maxmem
- \<currentMemory\>后面详细解释。



```
<controller type='pci' index='1' model='pci-expander-bus'>
  <model name='pxb'/>
  <target busNr='254'>
    <node>1</node>
  </target>
</controller>
```

上面xml表示一个pxb controller，其所属node1。



# 进阶

## host numa

上述我们讨论的所有参数只决定了guest的numa视图，并没有赋予实际意义：在guest中，node0的cpu访问node0的内存效率并不会比访问node1的内存高。

我们知道，guest的vcpu对应了host的一个线程，guest的ram对应host的ram。我们可以通过绑核，"绑内存"这样的操作，赋予guest的numa架构实际意义。



### 绑核

我们对vcpu对应的线程指定其运行所在的cpus的操作称之为绑核。在linux的实现是设置进程/线程的CPU亲和性(CPU affinity)。

进行vcpu绑核操作的是libvirt，所以qemu中并没有相关参数。

```
<vcpu placement='static' cpuset="1-4,^3,6" current="1">2</vcpu>

<cputune>
  <vcpupin vcpu="0" cpuset="1-4,^2"/>
  <vcpupin vcpu="1" cpuset="0,1"/>
  <vcpupin vcpu="2" cpuset="2,3"/>
  <vcpupin vcpu="3" cpuset="0,4"/>
<cputune>
```

\<vcpu\>中的cpuset和\<vcpupin\>指定了vpu和cpu的绑定关系，\<vcpupin\>优先级更高。

### taskset

taskset命令可以用于查看和设置进程/线程的CPU亲和性，具体man taskset。



### 绑内存

"绑内存"并不是一个通用的说法，本文中表示:指定numa节点申请内存。

guest内存的申请者是qemu，所以和绑核不同，"绑内存"是qemu进行的。其相关参数为：

```
-object memory-backend-ram,id=id,merge=on|off,dump=on|off,share=on|off,prealloc=on|off,size=size,host-nodes=host-nodes,policy=default|preferred|bind|interleave
```

```
-object memory-backend-memfd,id=id,merge=on|off,dump=on|off,share=on|off,prealloc=on|off,size=size,host-nodes=host-nodes,policy=default|preferred|bind|interleave,seal=on|off,hugetlb=on|off,hugetlbsize=size
```

memory-backend-ram/memory-backend-memfd表示一个memory backend object，子参数host-nodes&policy指定了其与host numa的关系：

host-nodes=node0,policy=bind表示该内存对象只使用host node0的内存。

示例：

```
-object memory-backend-ram,id=ram-node0,size=1073741824,host-nodes=0,policy=bind 
-numa node,nodeid=0,cpus=0,memdev=ram-node0 
-object memory-backend-ram,id=ram-node1,size=1073741824,host-nodes=1,policy=bind 
-numa node,nodeid=1,cpus=1,memdev=ram-node1 
-object memory-backend-ram,id=memdimm2,size=1073741824,host-nodes=0,policy=bind
-device pc-dimm,node=0,memdev=memdimm2,id=dimm2,slot=2 
```

注意在"绑内存"场景下，-numa必须使用memdev。



#### libvirt xml

```
<numatune>
  <memory mode='strict' nodeset='0-1'/>
  <memnode cellid='0' mode='strict' nodeset='0'/>
  <memnode cellid='1' mode='strict' nodeset='1'/>
</numatune>
```

- <memory\>设置整个guest的"绑内存"策略。
- <memnode\>提供<memory\>更精细化的控制，具体到node，<memnode\>优先级更高。
- \<memnode\>中cellid指定guest node。nodeset、mode对应上述的host-nodes&policy参数。



## memory balloon

memory balloon是guest中的一个特殊的pci 设备，host可以通过该设备控制guest系统使用的内存，本文不深入讨论其实现原理。

### balloon device

-device virtio-balloon-pci指令该设备。

```
-device virtio-balloon-pci,id=balloon0,bus=pci.1,addr=0x6 -msg timestamp=on
```

对应的xml：

```
<memballoon model='virtio'>
  <stats period='5'/>
  <address type='pci' domain='0x0000' bus='0x01' slot='0x06' function='0x0'/>
</memballoon>
```



### set/get balloon memory

使用hmp命令可以查看和设置balloon的值：

<img src="https://raw.githubusercontent.com/Reventon1993/pictures/master/typora/numa_pic3.png" alt="image-20231206163824459" style="zoom: 67%;" />

### balloon memory

对于一个如下规格的虚拟机：

```
-object memory-backend-ram,id=ram-node0,size=1073741824,host-nodes=0,policy=bind 
-numa node,nodeid=0,cpus=0,memdev=ram-node0 
-object memory-backend-ram,id=ram-node1,size=1073741824,host-nodes=1,policy=bind 
-numa node,nodeid=1,cpus=1,memdev=ram-node1 
-object memory-backend-ram,id=memdimm2,size=1073741824,host-nodes=0,policy=bind
-device pc-dimm,node=0,memdev=memdimm2,id=dimm2,slot=2 
```

当我们设置了balloon memory为1024后，在guest系统中查看相关内存信息：![image-20231206164603631](https://raw.githubusercontent.com/Reventon1993/pictures/master/typora/numa_pic4.png)

free显示系统内存只有1G左右，而numa信息里显示了系统内存为3G。

对于这种场景，个人认为1G为"OS的内存视图"下内存的容量，3G为"系统的内存试图"下内存的容量，设置balloon可以改变OS层面的内存视图(减少之后，内存会实际归还给host，这就是balloon的实际意义所在)。

**libvirt xml中的\<currentMemory\>设定了虚拟机开机时的balloon大小。其值需小于\<memory\>值。**

qemu并没有相关参数。



## 热升配

### 内存热升配

1.虚拟机在创建时设置较大\<memory\>，通过balloon热升配内存。

2.通过hotpluggable memory技术，热添加内存设备。



### CPU热升配

使用virsh setvcpus --live或者hmp的cpu-add命令。



# reference

https://en.wikipedia.org/wiki/Non-uniform_memory_access

https://stevescargall.com/blog/2022/11/03/linux-numa-distances-explained/

https://libvirt.org/formatdomain.html

https://www.qemu.org/docs/master/system/invocation.html
